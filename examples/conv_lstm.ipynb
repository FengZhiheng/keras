{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, CSVLogger\n",
    "from data_for_UNET import load_train_data, load_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from LoadDataHelper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myLoader = CTADataLoader('E:/CTAData/ALLDataINLabPCD')\n",
    "#imgs_train,labs_train = myLoader.loadData()#这里不建议使用Loader，因为没有做窗宽窗位的调整\n",
    "\n",
    "def pre_process():\n",
    "    imgs_train, labs_train = load_train_data()\n",
    "    imgs_train = np.transpose(imgs_train, (2, 0, 1, 3))\n",
    "\n",
    "    labs_train = np.transpose(labs_train, (2, 0, 1, 3))\n",
    "\n",
    "    imgs_train = imgs_train.astype(np.float32)\n",
    "    labs_train=labs_train.astype(np.float32)\n",
    "    return imgs_train,labs_train\n",
    "\n",
    "#load 原始数据\n",
    "imgs_train,labs_train = pre_process()\n",
    "\n",
    "#load cnn的结果\n",
    "#imgs_train = np.load('cnnPredectResutl.npy')\n",
    "#labs_train = np.load('cnnPredectLabels.npy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(582, 512, 512, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labs_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true,y_pred,epsilon=1e-8):\n",
    "    axes = tuple(range(1,len(y_pred.shape)-1))\n",
    "    intersection = 2.*K.sum(y_true*y_pred,axes)\n",
    "    return (intersection + epsilon)/(K.sum((K.square(y_true)+K.square(y_pred)),axes) + epsilon)\n",
    "\n",
    "def dice_coef_loss(y_ture,y_pred):\n",
    "    return 1-dice_coef(y_ture,y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1009 20:24:37.871079 13112 deprecation_wrapper.py:119] From c:\\users\\administrator\\anaconda3\\envs\\kerasenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1009 20:24:37.886707 13112 deprecation_wrapper.py:119] From c:\\users\\administrator\\anaconda3\\envs\\kerasenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:507: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1009 20:24:37.886707 13112 deprecation_wrapper.py:119] From c:\\users\\administrator\\anaconda3\\envs\\kerasenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3831: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1009 20:24:38.042977 13112 deprecation_wrapper.py:119] From c:\\users\\administrator\\anaconda3\\envs\\kerasenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:167: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1009 20:24:38.042977 13112 deprecation_wrapper.py:119] From c:\\users\\administrator\\anaconda3\\envs\\kerasenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W1009 20:24:42.137283 13112 deprecation_wrapper.py:119] From c:\\users\\administrator\\anaconda3\\envs\\kerasenv\\lib\\site-packages\\keras\\optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seq = Sequential()\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   input_shape=(None, 512, 512, 1),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(Conv3D(filters=1, kernel_size=(3, 3, 3),\n",
    "               activation='sigmoid',\n",
    "               padding='same', data_format='channels_last'))\n",
    "seq.compile(loss=dice_coef_loss, optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def generate_movies(n_samples=1200, n_frames=15):\n",
    "    row = 80\n",
    "    col = 80\n",
    "    noisy_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float)\n",
    "    shifted_movies = np.zeros((n_samples, n_frames, row, col, 1),\n",
    "                              dtype=np.float)\n",
    "    for i in range(n_samples):\n",
    "        # Add 3 to 7 moving squares\n",
    "        n = np.random.randint(3, 8)\n",
    "        for j in range(n):\n",
    "            # Initial position\n",
    "            xstart = np.random.randint(20, 60)\n",
    "            ystart = np.random.randint(20, 60)\n",
    "            # Direction of motion\n",
    "            directionx = np.random.randint(0, 3) - 1\n",
    "            directiony = np.random.randint(0, 3) - 1\n",
    "            # Size of the square\n",
    "            w = np.random.randint(2, 4)\n",
    "            for t in range(n_frames):\n",
    "                x_shift = xstart + directionx * t\n",
    "                y_shift = ystart + directiony * t\n",
    "                noisy_movies[i, t, x_shift - w: x_shift + w,\n",
    "                             y_shift - w: y_shift + w, 0] += 1\n",
    "                # Make it more robust by adding noise.\n",
    "                # The idea is that if during inference,\n",
    "                # the value of the pixel is not exactly one,\n",
    "                # we need to train the network to be robust and still\n",
    "                # consider it as a pixel belonging to a square.\n",
    "                if np.random.randint(0, 2):\n",
    "                    noise_f = (-1)**np.random.randint(0, 2)\n",
    "                    noisy_movies[i, t,\n",
    "                                 x_shift - w - 1: x_shift + w + 1,\n",
    "                                 y_shift - w - 1: y_shift + w + 1,\n",
    "                                 0] += noise_f * 0.1\n",
    "\n",
    "                # Shift the ground truth by 1\n",
    "                x_shift = xstart + directionx * (t + 1)\n",
    "                y_shift = ystart + directiony * (t + 1)\n",
    "                shifted_movies[i, t, x_shift - w: x_shift + w,\n",
    "                               y_shift - w: y_shift + w, 0] += 1\n",
    "    # Cut to a 40x40 window\n",
    "    noisy_movies = noisy_movies[::, ::, 20:60, 20:60, ::]\n",
    "    shifted_movies = shifted_movies[::, ::, 20:60, 20:60, ::]\n",
    "    noisy_movies[noisy_movies >= 1] = 1\n",
    "    shifted_movies[shifted_movies >= 1] = 1\n",
    "    return noisy_movies, shifted_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noisy_movies, shifted_movies = generate_movies(n_samples=1200, n_frames = 1)\n",
    "noisy_movies = imgs_train[::,np.newaxis, ::, ::,::]\n",
    "shifted_movies = labs_train[::,np.newaxis, ::, ::,::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(noisy_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(582, 1, 512, 512, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_movies.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#将数据保存到一个指定的目录下\n",
    "noisy_moviesPath = 'D:/Data/conv_lstm_data/noisy_movies/'\n",
    "shifted_moviesPath = 'D:/Data/conv_lstm_data/shifted_movies/'\n",
    "dataPath = 'D:/Data/conv_lstm_data/data/'\n",
    "#通过imageJ观察保存下载的图片\n",
    "#获取帧数\n",
    "frameNum = noisy_movies.shape[0]\n",
    "#写个for循环，保存每一帧\n",
    "for i in range(15):    \n",
    "    tmpNoiseImage = noisy_movies[1, i, ::, ::, 0]#取的是每一个视频中的第1帧\n",
    "    tmpShiftedImage = shifted_movies[1, i, ::, ::, 0]\n",
    "    # Let's look at the results.\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(tmpNoiseImage,cmap=\"magma\")\n",
    "    plt.axis('off') # 不显示坐标轴\n",
    "    plt.title('NoiseImage');\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(tmpShiftedImage,cmap=\"magma\")\n",
    "    plt.axis('off') # 不显示坐标轴\n",
    "    plt.title('ShiftedImage');\n",
    "    plt.savefig(dataPath+'%i_sourceVideo.png' % (i + 1))\n",
    "    \n",
    "def showArray(array,axis=0):\n",
    "    frameNum = array.shape[axis]\n",
    "    for i in range(frameNum):    \n",
    "        tmpNoiseImage = array[0, i, ::, ::, 0]#取的是每一个视频中的第1帧\n",
    "        plt.figure(figsize=(16,8))\n",
    "        plt.subplot(1,1,1)\n",
    "        plt.imshow(tmpNoiseImage,cmap=\"magma\")\n",
    "        plt.axis('off') # 不显示坐标轴\n",
    "        plt.title('Image');\n",
    "        which = 1004\n",
    "track = noisy_movies[which][:7, ::, ::, ::]\n",
    "aa = track[np.newaxis, ::, ::, ::, ::]\n",
    "aa.shape\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weightStoreFatherPath = './LSTMWeights/'\n",
    "weightStorePath=weightStoreFatherPath + 'weight.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(weightStorePath, verbose=1, monitor='val_loss', save_best_only=True)\n",
    "csv_logger = CSVLogger('./LSTMWeights/log.csv', append=True, separator='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1009 12:04:25.722305 15260 deprecation.py:323] From c:\\users\\administrator\\anaconda3\\envs\\kerasenv\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 552 samples, validate on 30 samples\n",
      "Epoch 1/100\n",
      "552/552 [==============================] - 97s 175ms/step - loss: 0.9971 - val_loss: 0.9985\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.99851, saving model to ./LSTMWeights/weightCNNLSTM.hdf5\n",
      "Epoch 2/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.9936 - val_loss: 0.9998\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.9645 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.9323 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.9995 - val_loss: 0.9968\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.99851 to 0.99682, saving model to ./LSTMWeights/weightCNNLSTM.hdf5\n",
      "Epoch 25/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.9897 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.9999 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.9999 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.9999 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.9999 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.9999 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.9998 - val_loss: 0.9997\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.5626 - val_loss: 0.9956\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.99682 to 0.99558, saving model to ./LSTMWeights/weightCNNLSTM.hdf5\n",
      "Epoch 43/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.4091 - val_loss: 0.9318\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.99558 to 0.93179, saving model to ./LSTMWeights/weightCNNLSTM.hdf5\n",
      "Epoch 44/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.3508 - val_loss: 0.9700\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.7907 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 1.0000 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.9530 - val_loss: 0.8366\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.93179 to 0.83665, saving model to ./LSTMWeights/weightCNNLSTM.hdf5\n",
      "Epoch 59/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.4413 - val_loss: 0.8326\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.83665 to 0.83263, saving model to ./LSTMWeights/weightCNNLSTM.hdf5\n",
      "Epoch 60/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.3600 - val_loss: 0.9018\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.3885 - val_loss: 0.9999\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.3273 - val_loss: 0.8256\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.83263 to 0.82565, saving model to ./LSTMWeights/weightCNNLSTM.hdf5\n",
      "Epoch 63/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.3319 - val_loss: 0.8169\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.82565 to 0.81691, saving model to ./LSTMWeights/weightCNNLSTM.hdf5\n",
      "Epoch 64/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.3189 - val_loss: 0.8572\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.3200 - val_loss: 0.8270\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.3179 - val_loss: 0.9992\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n",
      "Epoch 67/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.3237 - val_loss: 0.9765\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.3160 - val_loss: 0.8511\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.3049 - val_loss: 0.7966\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.81691 to 0.79660, saving model to ./LSTMWeights/weightCNNLSTM.hdf5\n",
      "Epoch 70/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.3081 - val_loss: 0.9999\n",
      "\n",
      "Epoch 00070: val_loss did not improve\n",
      "Epoch 71/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2986 - val_loss: 0.8029\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 72/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.3041 - val_loss: 0.8303\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 73/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.3157 - val_loss: 0.8109\n",
      "\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 74/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2912 - val_loss: 0.8325\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 75/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2998 - val_loss: 0.8323\n",
      "\n",
      "Epoch 00075: val_loss did not improve\n",
      "Epoch 76/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2929 - val_loss: 0.8087\n",
      "\n",
      "Epoch 00076: val_loss did not improve\n",
      "Epoch 77/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.3060 - val_loss: 0.8084\n",
      "\n",
      "Epoch 00077: val_loss did not improve\n",
      "Epoch 78/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.3011 - val_loss: 0.9999\n",
      "\n",
      "Epoch 00078: val_loss did not improve\n",
      "Epoch 79/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.3108 - val_loss: 0.9999\n",
      "\n",
      "Epoch 00079: val_loss did not improve\n",
      "Epoch 80/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2973 - val_loss: 0.9999\n",
      "\n",
      "Epoch 00080: val_loss did not improve\n",
      "Epoch 81/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2953 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00081: val_loss did not improve\n",
      "Epoch 82/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2911 - val_loss: 0.9999\n",
      "\n",
      "Epoch 00082: val_loss did not improve\n",
      "Epoch 83/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2939 - val_loss: 0.8172\n",
      "\n",
      "Epoch 00083: val_loss did not improve\n",
      "Epoch 84/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2867 - val_loss: 0.9999\n",
      "\n",
      "Epoch 00084: val_loss did not improve\n",
      "Epoch 85/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2899 - val_loss: 0.7976\n",
      "\n",
      "Epoch 00085: val_loss did not improve\n",
      "Epoch 86/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.3109 - val_loss: 0.8316\n",
      "\n",
      "Epoch 00086: val_loss did not improve\n",
      "Epoch 87/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2879 - val_loss: 0.8332\n",
      "\n",
      "Epoch 00087: val_loss did not improve\n",
      "Epoch 88/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2903 - val_loss: 0.8038\n",
      "\n",
      "Epoch 00088: val_loss did not improve\n",
      "Epoch 89/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2917 - val_loss: 0.8034\n",
      "\n",
      "Epoch 00089: val_loss did not improve\n",
      "Epoch 90/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2848 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00090: val_loss did not improve\n",
      "Epoch 91/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2851 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00091: val_loss did not improve\n",
      "Epoch 92/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2917 - val_loss: 0.8245\n",
      "\n",
      "Epoch 00092: val_loss did not improve\n",
      "Epoch 93/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2794 - val_loss: 0.8004\n",
      "\n",
      "Epoch 00093: val_loss did not improve\n",
      "Epoch 94/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2825 - val_loss: 0.8066\n",
      "\n",
      "Epoch 00094: val_loss did not improve\n",
      "Epoch 95/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2920 - val_loss: 0.8192\n",
      "\n",
      "Epoch 00095: val_loss did not improve\n",
      "Epoch 96/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2784 - val_loss: 0.9999\n",
      "\n",
      "Epoch 00096: val_loss did not improve\n",
      "Epoch 97/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2854 - val_loss: 0.8113\n",
      "\n",
      "Epoch 00097: val_loss did not improve\n",
      "Epoch 98/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2887 - val_loss: 0.9999\n",
      "\n",
      "Epoch 00098: val_loss did not improve\n",
      "Epoch 99/100\n",
      "552/552 [==============================] - 87s 158ms/step - loss: 0.2861 - val_loss: 0.8197\n",
      "\n",
      "Epoch 00099: val_loss did not improve\n",
      "Epoch 100/100\n",
      "550/552 [============================>.] - ETA: 0s - loss: 0.2862"
     ]
    }
   ],
   "source": [
    "seq.fit(noisy_movies, shifted_movies, batch_size=10,\n",
    "        epochs=100, validation_split=0.05,callbacks=[model_checkpoint,csv_logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将训练好的模型保存起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(582, 1, 512, 512, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_movies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用训练好的网络进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "PredectionsImagesPath = './LSTMResults/PredectionsImages/'\n",
    "MaskImagesPath ='./LSTMResults/LabelsImages/'\n",
    "# matplotlib.image.imsave(PredectionsImagesPath+'name.png', oneimage)\n",
    "weightStorePath =  './LSTMWeights/'+'weight.hdf5'\n",
    "seq.load_weights(weightStorePath)\n",
    "for i in range(noisy_movies.shape[0]):\n",
    "    which = i\n",
    "    track = noisy_movies[which][::, ::, ::, ::]    \n",
    "    new_pos = seq.predict(track[np.newaxis, ::, ::, ::, ::])\n",
    "    track2 = shifted_movies[which][::, ::, ::, ::]\n",
    "    \n",
    "    \n",
    "    cv2.imwrite(PredectionsImagesPath+'%i.png'%(which+1), new_pos[0, 0, ::, ::, 0])\n",
    "    cv2.imwrite(MaskImagesPath+'%i.png'%(which+1), track2[0, ::, ::, 0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     from matplotlib import pyplot as plt\n",
    "#     fig = plt.figure(figsize=(10, 5))\n",
    "#     ax = fig.add_subplot(131)\n",
    "#     ax.text(1, 3, 'LSTM Predictions', fontsize=20)\n",
    "#     toplot = new_pos[0, 0, ::, ::, 0]\n",
    "#     plt.imshow(toplot)\n",
    "#     ax = fig.add_subplot(132)\n",
    "#     plt.text(1, 3, 'Ground Truth', fontsize=20)\n",
    "#     toplot = track2[0, ::, ::, 0]\n",
    "#     plt.imshow(toplot)\n",
    "#     ax = fig.add_subplot(133)\n",
    "#     plt.text(1, 3, 'Original Image', fontsize=20)\n",
    "#     toplot = track[0, ::, ::, 0]\n",
    "#     plt.imshow(toplot)\n",
    "#     plt.savefig(dataPath+'%i_animate.png' % (which + 1))\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneimage = new_pos[0, 0, ::, ::, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneimage.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 512, 512, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512, 512, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
